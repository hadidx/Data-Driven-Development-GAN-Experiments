{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bda477",
   "metadata": {},
   "source": [
    "# Creating Synthetic Vertebra CT Scans from Segmentation Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92fdb5",
   "metadata": {},
   "source": [
    "1) Install the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa375c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nibabel\n",
      "  Downloading nibabel-5.2.1-py3-none-any.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 114.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from nibabel) (1.23.5)\n",
      "Requirement already satisfied: packaging>=17 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from nibabel) (23.2)\n",
      "Installing collected packages: nibabel\n",
      "Successfully installed nibabel-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2d1fca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nPerlinNoise\n",
      "  Downloading nPerlinNoise-0.1.4a0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy>=1.23.3 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from nPerlinNoise) (1.23.5)\n",
      "Requirement already satisfied: numexpr>=2.8.3 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from nPerlinNoise) (2.8.4)\n",
      "Installing collected packages: nPerlinNoise\n",
      "Successfully installed nPerlinNoise-0.1.4a0\n"
     ]
    }
   ],
   "source": [
    "!pip install nPerlinNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29db5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrivis\n",
      "  Downloading mrivis-0.3.8-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from mrivis) (1.10.0)\n",
      "Requirement already satisfied: nibabel in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from mrivis) (5.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from mrivis) (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from mrivis) (3.7.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (23.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from matplotlib->mrivis) (9.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hadia\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mrivis) (1.16.0)\n",
      "Installing collected packages: mrivis\n",
      "Successfully installed mrivis-0.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install mrivis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791ca35",
   "metadata": {},
   "source": [
    "2) Write the needed opencv helper functions. These functions do a lot of useful things like find the contours, errode and dilate, find boundaries of contours, generate and apply noise, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b47e29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from perlin_noise import PerlinNoise\n",
    "from NPerlinNoise import *\n",
    "from mrivis import SlicePicker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def contourIsWide(contour):\n",
    "    _,_,w,h = cv2.boundingRect(contour)\n",
    "    if w/h > 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def keepContoursOnSameLineHavingLargestArea(contours):\n",
    "    groups = getContourGroups(contours)\n",
    "    contours_to_keep = []\n",
    "    for group in groups:\n",
    "        group_contours = group['contours']\n",
    "        \n",
    "        areas = []\n",
    "        \n",
    "        for contour in group_contours:\n",
    "            areas.append(cv2.contourArea(contour))\n",
    "        \n",
    "        best_idx = areas.index(max(areas))\n",
    "        \n",
    "        contours_to_keep.append(group_contours[best_idx])\n",
    "        \n",
    "    return contours_to_keep\n",
    "\n",
    "def seperateContoursVerticallyAndKeepLargestArea(contours):\n",
    "    cxs = []\n",
    "    for contour in contours:\n",
    "        cx,cy = getControurCentroid(contour)\n",
    "        cxs.append(cx)\n",
    "    average_cx = sum(cxs)/len(cxs)\n",
    "    \n",
    "    left_group = []\n",
    "    right_group = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        cx,cy = getControurCentroid(contour)\n",
    "        if cx<average_cx:\n",
    "            left_group.append(contour)\n",
    "        else:\n",
    "            right_group.append(contour)\n",
    "    \n",
    "    left_area = sum([cv2.contourArea(contour) for contour in left_group])\n",
    "    right_area = sum([cv2.contourArea(contour) for contour in right_group])\n",
    "    \n",
    "    if left_area>right_area:\n",
    "        return left_group\n",
    "    return right_group\n",
    "\n",
    "def getBestSlice(data, dim = 1):\n",
    "    slices = SlicePicker(data, dim)\n",
    "    slices_list = [s[1] for s in list(slices)]\n",
    "    try:\n",
    "        return slices_list[len(slices_list)//2]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def getBestContours(image):\n",
    "    kernel_sizes = [0, 5, 8, 10]\n",
    "    num_contours_for_kernels = [0]*len(kernel_sizes)\n",
    "    \n",
    "    for i in range(len(kernel_sizes)):\n",
    "        _, thresh = cv2.threshold(image, 100, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        kernel = np.ones((kernel_sizes[i], kernel_sizes[i]), np.uint8)\n",
    "        opened = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "        contours, _ = cv2.findContours(opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        num_contours_for_kernels[i] = len(contours)\n",
    "       \n",
    "    best_kernel_idx =  num_contours_for_kernels.index(max(num_contours_for_kernels))\n",
    "    \n",
    "    _, thresh = cv2.threshold(image, 100, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((kernel_sizes[best_kernel_idx], kernel_sizes[best_kernel_idx]), np.uint8)\n",
    "    opened = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    contours, _ = cv2.findContours(opened, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return contours\n",
    "\n",
    "##apply the sigmoid function with a translation and power. Useful to create a step function that zeros out all values below a threshold.\n",
    "## used to create the boundary of the contours.\n",
    "def customSigmoid(matrix, power, translation):\n",
    "    return 1/(1 + np.exp(-(matrix-translation))**power)\n",
    "## define a random perlin noise\n",
    "def getNoise():\n",
    "    return Noise(\n",
    "        seed=None,\n",
    "        frequency=8,\n",
    "        waveLength=64,\n",
    "        warp=None,\n",
    "        _range=None,\n",
    "        octaves=8,\n",
    "        persistence=0.5,\n",
    "        lacunarity=2\n",
    "    )\n",
    "\n",
    "def getControurCentroid(contour):\n",
    "    M = cv2.moments(contour)\n",
    "    if M['m00']:\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "    else:\n",
    "        cx, cy = (0,0)\n",
    "    \n",
    "    return cx,cy\n",
    "\n",
    "def getRelativeContourCentroid(contour):\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Calculate the centroid of the contour\n",
    "    M = cv2.moments(contour)\n",
    "    if M['m00']:\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "    else:\n",
    "        cx, cy = (0,0)\n",
    "    \n",
    "    # Calculate the centroid relative to the top-left edge of the contour\n",
    "    rel_cx = cx - x\n",
    "    rel_cy = cy - y\n",
    "    \n",
    "    return rel_cx, rel_cy\n",
    "\n",
    "## this follows a huristic to keep only contours that have no neighbors to their left or right. Makes identifying vertebra easier when looking at the front CT slice.\n",
    "def eliminateContoursOnSameLine(contours):\n",
    "    contours_not_on_same_line = []\n",
    "    \n",
    "    contour_centroids = [getControurCentroid(contour) for contour in contours]\n",
    "    for i in range(len(contours)):\n",
    "        add_contour = True\n",
    "        for j in range(len(contours)):\n",
    "            if i!=j and abs(contour_centroids[i][1] - contour_centroids[j][1]) < 10:\n",
    "                add_contour = False\n",
    "                break\n",
    "        if add_contour:\n",
    "            contours_not_on_same_line.append(contours[i])\n",
    "    \n",
    "    contours_not_having_gaps = []\n",
    "    \n",
    "    contour_centroids = [getControurCentroid(contour) for contour in contours_not_on_same_line]\n",
    "    \n",
    "    for i in range(len(contours_not_on_same_line)):\n",
    "        add_contour = True\n",
    "        if i>0 and abs(contour_centroids[i][1] - contour_centroids[i-1][1]) > 70:\n",
    "            add_contour = False\n",
    "        if i<len(contours_not_on_same_line)-1 and abs(contour_centroids[i][1] - contour_centroids[i+1][1]) > 70:\n",
    "            add_contour = False\n",
    "            contours_not_having_gaps = []\n",
    "        if add_contour:\n",
    "            contours_not_having_gaps.append(contours_not_on_same_line[i])\n",
    "    \n",
    "    return contours_not_having_gaps\n",
    "\n",
    "\n",
    "## what constitutes a contour group is a group of bones that have close centroids on the y-axis\n",
    "def getContourGroups(contours):\n",
    "    contour_groups = []\n",
    "    for contour in contours:\n",
    "        cx, cy = getControurCentroid(contour)\n",
    "        # Check if the contour belongs to an existing group\n",
    "        group_found = False\n",
    "        for group in contour_groups:\n",
    "            meanCentroid = np.array(group['centroids']).mean(0)\n",
    "            ##this used to be 20 but I changed it later!!\n",
    "            if abs(meanCentroid[1] - cy) < 25:\n",
    "                group['contours'].append(contour)\n",
    "                group['centroids'].append((cx, cy))\n",
    "                group_found = True\n",
    "                break\n",
    "\n",
    "        # If the contour doesn't belong to an existing group, create a new group\n",
    "        if not group_found:\n",
    "            contour_groups.append({'contours': [contour], 'centroids': [(cx, cy)]})\n",
    "            \n",
    "    return contour_groups\n",
    "\n",
    "#draws the border of the conrour group and applies the noise h\n",
    "def drawContourGroupCT(group, mask, h):\n",
    "    # Create a filled contour\n",
    "    cv2.drawContours(mask, [group], 0, 255, -2)\n",
    "\n",
    "    # Create a distance transform\n",
    "    dist_transform = cv2.distanceTransform(mask, cv2.DIST_L2, 5)\n",
    "\n",
    "    # Normalize the distance transform\n",
    "    norm_dist_transform = cv2.normalize(dist_transform, None, 0, 1.0, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Generate a gradient using the normalized distance transform\n",
    "    gradient = customSigmoid(1 - norm_dist_transform, 20, 0.8) * 255\n",
    "    gradient += customSigmoid(norm_dist_transform, 30, 0.9) * 255 * h\n",
    "    gradient = np.minimum(gradient, 255)\n",
    "    gradient = gradient.astype(np.uint8)\n",
    "\n",
    "    rows, cols = gradient.shape\n",
    "\n",
    "#     rand = np.random.random(image.shape)*50\n",
    "#     gradient = np.minimum(np.maximum((gradient + rand),0),255)\n",
    "    gradient = np.minimum(np.maximum((gradient + h*255),0),255)\n",
    "\n",
    "#     for i in range(rows):\n",
    "#         for j in range(cols):\n",
    "#             noise_val = h[i, j]\n",
    "#             gradient[i, j] = max(0, min(255, int(noise_val*gradient[i, j])))\n",
    "\n",
    "    # Apply the gradient to the contour region\n",
    "    mask[mask == 255] = gradient[mask == 255]\n",
    "\n",
    "#this is just for demonstration purpuses. same as above basically\n",
    "def drawContourGroupCTShowSteps(group, image, h):\n",
    "    # Create a filled contour\n",
    "    mask0 = np.zeros_like(image)\n",
    "\n",
    "    res1, res2 = mask0.shape\n",
    "    mask1 = np.zeros_like(image)\n",
    "\n",
    "    res1, res2 = mask1.shape\n",
    "    \n",
    "    mask2 = np.zeros_like(image)\n",
    "\n",
    "    res1, res2 = mask1.shape\n",
    "    \n",
    "    cv2.drawContours(mask1, [group], 0, 255, -2)\n",
    "\n",
    "    # Create a distance transform\n",
    "    dist_transform = cv2.distanceTransform(mask1, cv2.DIST_L2, 5)\n",
    "#     dist_transform = cv2.distanceTransform(mask2, cv2.DIST_L2, 5)\n",
    "\n",
    "    # Normalize the distance transform\n",
    "    norm_dist_transform = cv2.normalize(dist_transform, None, 0, 1.0, cv2.NORM_MINMAX)\n",
    "    contrast_factor = np.random.uniform(0.2, 1)  # Adjust the range as needed\n",
    "\n",
    "    h = cv2.convertScaleAbs(h*255, alpha=contrast_factor, beta=1)/255\n",
    "    # Generate a gradient using the normalized distance transform\n",
    "    gradient = customSigmoid(1 - norm_dist_transform, 20, 0.8) * 255\n",
    "    mask1[mask1 == 255] = gradient[mask1 == 255]\n",
    "    gradient += customSigmoid(norm_dist_transform, 30, 0.9) * 255 * h\n",
    "    gradient = np.minimum(gradient, 255)\n",
    "    gradient = gradient.astype(np.uint8)\n",
    "\n",
    "    rows, cols = gradient.shape\n",
    "\n",
    "#     rand = np.random.random(image.shape)*50\n",
    "#     gradient = np.minimum(np.maximum((gradient + rand),0),255)\n",
    "    gradient = np.minimum(np.maximum((gradient + h*255),0),255)\n",
    "\n",
    "#     for i in range(rows):\n",
    "#         for j in range(cols):\n",
    "#             noise_val = h[i, j]\n",
    "#             gradient[i, j] = max(0, min(255, int(noise_val*gradient[i, j])))\n",
    "\n",
    "    # Apply the gradient to the contour region\n",
    "    cv2.drawContours(mask2, [group], 0, 255, -2)\n",
    "    mask2[mask2 == 255] = gradient[mask2 == 255]\n",
    "    \n",
    "    x, y, w, h = cv2.boundingRect(group)\n",
    "    offset = 100\n",
    "    cv2.drawContours(mask0, [group], 0, 255, -2)\n",
    "    mask0 = mask0[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "    mask1 = mask1[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "    mask2 = mask2[y-offset:y+h+offset, x-offset:x+w+offset]\n",
    "    plt.imshow(mask0, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(mask1, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(mask2, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f304c9",
   "metadata": {},
   "source": [
    "3) Use the helper functions to create the transformation needed and save the images that were produced from the original CT Scan file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3076f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformSegmentationToCTs(image, save_folder, prefix):\n",
    "    noise = getNoise()\n",
    "\n",
    "    _, thresh = cv2.threshold(image, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    contour_groups = getContourGroups(contours)\n",
    "    for i in range(len(contour_groups)):\n",
    "\n",
    "        extendedContours = []\n",
    "        #Commented loops below if generating extended vertebra CTs \n",
    "#         if i == 0 or i == len(contour_groups) -1:\n",
    "#             for contour in contour_groups[i]['contours']:\n",
    "#                 _,_,w,height = cv2.boundingRect(contour)\n",
    "#                 if height>20:\n",
    "#                     extendedContours.append(contour)\n",
    "#         else:\n",
    "#             for j in range(i-1,i+2):\n",
    "#                 for contour in contour_groups[j]['contours']:\n",
    "#                     extendedContours.append(contour)\n",
    "\n",
    "        #loop below if generating single vertebra CTs\n",
    "        for contour in contour_groups[i]['contours']:\n",
    "                _,_,w,height = cv2.boundingRect(contour)\n",
    "                if height>20:\n",
    "                    extendedContours.append(contour)\n",
    "                    \n",
    "        if len(extendedContours) == 0:\n",
    "            continue\n",
    "        \n",
    "        mask = np.zeros_like(image)\n",
    "\n",
    "        res1, res2 = mask.shape\n",
    "\n",
    "        gradients = Gradient.scope()\n",
    "        h, coordsMesh = perlinGenerator(noise,(0, res1),(0, res2))\n",
    "        \n",
    "        h = h[:res1,:res2]\n",
    "        \n",
    "        # Change noise contrast\n",
    "        contrast_factor = np.random.uniform(0.2, 1)  # Adjust the range as needed\n",
    "\n",
    "        h = cv2.convertScaleAbs(h*255, alpha=contrast_factor, beta=1)/255\n",
    "\n",
    "        for cnt in extendedContours:\n",
    "            drawContourGroupCT(cnt, mask, h)\n",
    "\n",
    "        first_cx, first_cy = getRelativeContourCentroid(extendedContours[0])\n",
    "\n",
    "        last_cx, last_cy = getRelativeContourCentroid(extendedContours[-1])\n",
    "\n",
    "        all_contours = np.concatenate(extendedContours)\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(all_contours)\n",
    "        cropped_image = None\n",
    "        #Commented crop below if generating extended vertebra CTs\n",
    "#         if i == 0 or i == len(contour_groups) -1:\n",
    "#             cropped_image = mask[y:y+h, x:x+w]\n",
    "#         else:\n",
    "#             cropped_image = mask[y+last_cy:y+h-first_cy, x:x+w]\n",
    "\n",
    "        #Crop below if for generating single vertebra CTs\n",
    "        cropped_image = mask[y:y+h, x:x+w]\n",
    "        try:\n",
    "            cv2.imwrite(save_folder + '/' + prefix+'_'+str(i) +\".png\", cropped_image)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86ca50",
   "metadata": {},
   "source": [
    "4) loop over all nii files from the CTSpine1K dataset, get the best front facing slice index, and then get the best contours from this slice and apply the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3718c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1005/1005 [34:56<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "directory_name = 'data'\n",
    "directory = os.fsencode(directory_name)\n",
    "save_directory_name = 'seg2ct3'    \n",
    "\n",
    "for file in tqdm(os.listdir(directory)):\n",
    "    filename = os.fsdecode(file)\n",
    "    path = directory_name+'/'+filename\n",
    "    # Load NIfTI file\n",
    "    nii_file = nib.load(path)\n",
    "    # Get the data array\n",
    "    data = nii_file.get_fdata()\n",
    "#     slice_index = 256  # Change this to the desired slice index\n",
    "#     image = data[:, slice_index, ::-1].T.astype(np.uint8)*255\n",
    "#     if image.max() == 0:\n",
    "    slice_index = getBestSlice(data)\n",
    "    if not slice_index:\n",
    "        continue\n",
    "    image = data[:, slice_index, ::-1].T.astype(np.uint8)*255\n",
    "    best_contours =  getBestContours(image)\n",
    "    wanted_contours = eliminateContoursOnSameLine(best_contours)\n",
    "    image = np.zeros_like(image)\n",
    "    for contour in wanted_contours:\n",
    "        cv2.drawContours(image, [contour], 0, 255, -2)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "    transformSegmentationToCTs(image, save_directory_name, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b6bf2",
   "metadata": {},
   "source": [
    "5) same as above except it looks at the best side facing slice, gets the contours, and applies the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61aa53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1005/1005 [1:44:46<00:00,  6.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "directory_name = 'data'\n",
    "directory = os.fsencode(directory_name)\n",
    "save_directory_name = 'seg2ct3/side'    \n",
    "\n",
    "for file in tqdm(os.listdir(directory)):\n",
    "    filename = os.fsdecode(file)\n",
    "    path = directory_name+'/'+filename\n",
    "    # Load NIfTI file\n",
    "    nii_file = nib.load(path)\n",
    "    # Get the data array\n",
    "    data = nii_file.get_fdata()\n",
    "    slice_index = getBestSlice(data, 0)\n",
    "    if not slice_index:\n",
    "        continue\n",
    "    image = data[slice_index, :, ::-1].T.astype(np.uint8)*255\n",
    "    best_contours =  getBestContours(image)\n",
    "    wanted_contours = seperateContoursVerticallyAndKeepLargestArea(best_contours)\n",
    "    wanted_contours = keepContoursOnSameLineHavingLargestArea(wanted_contours)\n",
    "    image = np.zeros_like(image)\n",
    "    for contour in wanted_contours:\n",
    "        cv2.drawContours(image, [contour], 0, 255, -2)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "    transformSegmentationToCTs(image, save_directory_name, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f0c14",
   "metadata": {},
   "source": [
    "6) apply padding to all the generated images so that they are centered in a 128x128 pixel image. We first apply some checks such as trying to fit a polygon and counting the verticies, and checking the width to height ratio. This is done to eliminate some weird images that we couldn't eliminate with the huristics we applied earier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f105c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3262/3262 [00:01<00:00, 2009.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "directory_name = 'seg2ct3'\n",
    "directory = os.fsencode(directory_name)\n",
    "save_directory_name = 'padded_seg2ct3'    \n",
    "\n",
    "for file in tqdm(os.listdir(directory)):\n",
    "    try:\n",
    "        filename = os.fsdecode(file)\n",
    "        path = directory_name+'/'+filename\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        _, thresh = cv2.threshold(img, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if len(contours)>1:\n",
    "            continue\n",
    "        \n",
    "        for cnt in contours:\n",
    "            epsilon = 5\n",
    "            approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "        \n",
    "        if len(approx)>6:\n",
    "            continue\n",
    "\n",
    "        h,w = img.shape\n",
    "        \n",
    "        if h/w>1:\n",
    "            continue\n",
    "\n",
    "        padded_img = np.zeros((128,128))\n",
    "        h1,w1 = padded_img.shape\n",
    "    \n",
    "        padded_img[(h1-h)//2:(h1+h)//2, (w1-w)//2:(w1+w)//2] = img\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cv2.imwrite(save_directory_name + '/' + filename, padded_img)\n",
    "    except:\n",
    "        pass\n",
    "    # Load NIfTI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f560b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6726/6726 [00:03<00:00, 1874.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "directory_name = 'seg2ct3/side'\n",
    "directory = os.fsencode(directory_name)\n",
    "save_directory_name = 'padded_seg2ct3'    \n",
    "\n",
    "for file in tqdm(os.listdir(directory)):\n",
    "    try:\n",
    "        filename = os.fsdecode(file)\n",
    "        path = directory_name+'/'+filename\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        h,w = img.shape\n",
    "        \n",
    "        _, thresh = cv2.threshold(img, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if len(contours)>1:\n",
    "            continue\n",
    "        \n",
    "        for cnt in contours:\n",
    "            epsilon = 5\n",
    "            approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "        \n",
    "        if len(approx)>6:\n",
    "            continue\n",
    "\n",
    "        h,w = img.shape\n",
    "        \n",
    "        if h/w>1:\n",
    "            continue\n",
    "\n",
    "        padded_img = np.zeros((128,128))\n",
    "        h1,w1 = padded_img.shape\n",
    "    \n",
    "        padded_img[(h1-h)//2:(h1+h)//2, (w1-w)//2:(w1+w)//2] = img\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        cv2.imwrite(save_directory_name + '/side_' + filename, padded_img)\n",
    "    except:\n",
    "        pass\n",
    "    # Load NIfTI file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891236b0",
   "metadata": {},
   "source": [
    "7) (Unrelated) Here we take the AUBMC 256x256 pixel images, and crop them down to 128x128 pixels. This is done just so that the AUBMC images are the same size as the images we generated, and to reduce the amount of data we need to generate by the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b97dd816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 752.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "directory_name = 'cropped_good'\n",
    "directory = os.fsencode(directory_name)\n",
    "save_directory_name = 'cropped_healthy_vertebra'    \n",
    "\n",
    "for file in tqdm(os.listdir(directory)):\n",
    "    filename = os.fsdecode(file)\n",
    "    path = directory_name+'/'+filename\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    width, height = img.shape[1], img.shape[0]\n",
    "\n",
    "    mid_x, mid_y = int(width/2), int(height/2)\n",
    "    crop_offset = int(128//2)\n",
    "    crop_img = img[mid_y-crop_offset:mid_y+crop_offset, mid_x-crop_offset:mid_x+crop_offset]\n",
    "    try:\n",
    "        cv2.imwrite(save_directory_name + '/' + filename, crop_img)\n",
    "    except:\n",
    "        pass\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbb1e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 663.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "directory_name = 'cropped_bad'\n",
    "directory = os.fsencode(directory_name)\n",
    "save_directory_name = 'cropped_fractured_vertebra'    \n",
    "\n",
    "for file in tqdm(os.listdir(directory)):\n",
    "    filename = os.fsdecode(file)\n",
    "    path = directory_name+'/'+filename\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    width, height = img.shape[1], img.shape[0]\n",
    "\n",
    "    mid_x, mid_y = int(width/2), int(height/2)\n",
    "    crop_offset = int(128//2)\n",
    "    crop_img = img[mid_y-crop_offset:mid_y+crop_offset, mid_x-crop_offset:mid_x+crop_offset]\n",
    "    try:\n",
    "        cv2.imwrite(save_directory_name + '/' + filename, crop_img)\n",
    "    except:\n",
    "        pass\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
